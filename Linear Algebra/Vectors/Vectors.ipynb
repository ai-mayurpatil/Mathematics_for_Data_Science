{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vectors in Data Science**\n",
    "\n",
    "### **What is a Vector?**  \n",
    "A vector is an ordered collection of numbers (scalars) that represent a point in space. Mathematically, a vector is an **n-dimensional array** of numerical values.\n",
    "\n",
    "Example:  \n",
    "$$\n",
    "v = \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}\n",
    "$$\n",
    "This is a **3D vector**, representing a point in three-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications of Vectors in Data Science**\n",
    "\n",
    "### **1. Feature Representation in Machine Learning**\n",
    "- In supervised learning, data points are often represented as vectors.\n",
    "- Each feature in a dataset corresponds to a dimension in a vector.\n",
    "- Example: A house price prediction model may use a feature vector:  \n",
    "  $$\n",
    "  v = \\begin{bmatrix} \\text{size} \\\\ \\text{bedrooms} \\\\ \\text{age} \\end{bmatrix} = \\begin{bmatrix} 2000 \\\\ 3 \\\\ 15 \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "### **2. Word Embeddings in NLP**\n",
    "- Words or phrases can be represented as vectors in high-dimensional space.\n",
    "- Word2Vec, GloVe, and BERT transform words into vector representations.\n",
    "- Example: The word \"king\" might be represented as a 300-dimensional vector.\n",
    "\n",
    "### **3. Image Processing & Computer Vision**\n",
    "- Images are represented as vectors by flattening pixel values.\n",
    "- Example: A grayscale **28Ã—28** image is converted into a **784-dimensional vector**.\n",
    "\n",
    "### **4. Recommendation Systems**\n",
    "- User preferences and item features are represented as vectors.\n",
    "- Cosine similarity is often used to measure the similarity between users/items.\n",
    "\n",
    "### **5. Dimensionality Reduction (PCA, t-SNE)**\n",
    "- High-dimensional data is projected into lower dimensions for visualization and efficiency.\n",
    "- Principal Component Analysis (PCA) transforms high-dimensional vectors into a smaller set of uncorrelated components.\n",
    "\n",
    "### **6. Clustering (K-Means, DBSCAN)**\n",
    "- Clustering algorithms work by grouping similar data points based on vector representations.\n",
    "- Example: Customer segmentation using K-Means clustering.\n",
    "\n",
    "### **7. Deep Learning and Neural Networks**\n",
    "- Input layers in neural networks take vectors as input.\n",
    "- Feature extraction layers convert raw data into meaningful vector representations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Operations on Vectors**\n",
    "Vectors support operations that help in understanding relationships between data points.\n",
    "\n",
    "### **1. Addition & Subtraction**  \n",
    "$$\n",
    "\\mathbf{a} + \\mathbf{b} = (a_1 + b_1, a_2 + b_2, \\dots, a_n + b_n)\n",
    "$$\n",
    "\n",
    "### **2. Dot Product (Similarity Measure)**\n",
    "- Used in **cosine similarity** and **linear regression**.\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i\n",
    "$$\n",
    "\n",
    "### **3. Magnitude (Norm)**\n",
    "- Measures vector length.\n",
    "$$\n",
    "\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n",
    "$$\n",
    "\n",
    "### **4. Cosine Similarity**\n",
    "- Measures similarity between two vectors.\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
    "$$\n",
    "- Used in **document similarity, recommendation systems,** etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
